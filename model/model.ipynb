{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sTAjn9eBhhJ8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader    # dataset representation and loading\n",
        "import torch.autograd as autograd         # computation graph\n",
        "from torch import Tensor                  # tensor node in the computation graph\n",
        "import torch.nn as nn                     # neural networks\n",
        "import torch.nn.functional as F           # layers, activations and more\n",
        "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
        "from torch.jit import script, trace       # hybrid frontend decorator and tracing jit\n",
        "import numpy as np\n",
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CSVDataset(Dataset): \n",
        "    # 9 feats: trip count, num ops, num operands, num mem ops, num fops, num branches, est resmii, frequent path length, depth of loop\n",
        "\n",
        "    def __init__(self, csv_path):\n",
        "        csv_reader = csv.reader(open(csv_path, 'r'), delimiter=',')\n",
        "\n",
        "        self.csv_lines = []\n",
        "        for line in csv_reader:\n",
        "            self.csv_lines.append(line)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.csv_lines)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # line = [prob_name, f1, ... , f9, label, rank1, ..., rank8, time1, ..., time8]\n",
        "        line = self.csv_lines[idx]\n",
        "        X = torch.FloatTensor(line[1:10])\n",
        "        y = line[10]\n",
        "        return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUoVVslNghrg"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self,input_size,hidden_dim_1,hidden_dim_2,num_classes,drop_prob=0.5):\n",
        "        super(MLP, self).__init__()\n",
        "        #Put GeneModel architecture here (WITHOUT THE FINAL LAYERS)\n",
        "        self.hidden_dim_1=hidden_dim_1\n",
        "        self.hidden_dim_2=hidden_dim_2\n",
        "        self.num_classes=num_classes\n",
        "        \n",
        "        #self.flantten=torch.flatten()\n",
        "        self.fc_1=nn.Linear(input_size,hidden_dim_1)\n",
        "        self.fc_2=nn.Linear(hidden_dim_1,hidden_dim_2)\n",
        "        self.fc_3=nn.Linear(hidden_dim_2,num_classes)\n",
        "\n",
        "        self.relu = nn.LeakyReLU()\n",
        "        self.dropout = nn.Dropout(p=drop_prob)\n",
        "\n",
        "        self.loss_function = nn.CrossEntropyLoss()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        #Put GeneModel architecture here (WITHOUT THE FINAL LAYERS)\n",
        "        x=x.float()\n",
        "        #x=torch.flatten(x).float()\n",
        "        x=self.fc_1(x)\n",
        "        x=self.relu(x)\n",
        "        x=self.fc_2(x)\n",
        "        x=self.relu(x)\n",
        "        x=self.fc_3(x)\n",
        "        return x\n",
        "    \n",
        "    # def train(self, trainloader, optimizer, n_epochs = 30):\n",
        "    #     train_loss = 0\n",
        "    #     losses_epochs = []\n",
        "    #     for epoch in range(n_epochs):\n",
        "    #         print(\"Starting epoch: \", epoch)\n",
        "    #         for X, y in train_batch:\n",
        "    #             optimizer.zero_grad()\n",
        "\n",
        "    #             outputs = self(X1, X2)\n",
        "    #             loss = self.criterion(outputs, y)\n",
        "    #             loss.backward()\n",
        "    #             optimizer.step()\n",
        "    #             train_loss += loss.item()\n",
        "    #         print(\"train loss:\", train_loss)\n",
        "    #         losses_epochs.append(train_loss)\n",
        "    #         train_loss = 0\n",
        "    #     return losses_epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5UM0JlKhMz7"
      },
      "outputs": [],
      "source": [
        "mlp = MLP(8,8,8,8)\n",
        "train_csv = \"mock_train.csv\"\n",
        "val_csv = \"mock_val.csv\"\n",
        "\n",
        "lr = 0.001\n",
        "optimizer = optim.Adam(mlp.parameters(), lr=lr)\n",
        "epochs = 30\n",
        "batch_size = 32\n",
        "\n",
        "# Dataloader\n",
        "train_dataset = CSVDataset(train_csv)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataset = CSVDataset(val_csv)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for epoch in epochs:\n",
        "    for X, y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        pred = mlp(X)\n",
        "        loss = mlp.loss_function(pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    mlp.eval()\n",
        "    for X, y in val_loader:\n",
        "        pred = mlp(X)\n",
        "        val_loss = mlp.loss_function(pred, y)\n",
        "    \n",
        "    "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
